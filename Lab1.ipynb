{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Research Domain-Edtech/Learning tools\n",
        "The EdTech domain focuses on leveraging natural language processing to enhance digital learning tools by analyzing educational content, personalizing student experiences, and improving instructional feedback through automated text processing techniques.\n"
      ],
      "metadata": {
        "id": "0Nb4R7PEGxsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet spacy sentencepiece\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "import re, sentencepiece as spm\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "from nltk.tokenize import word_tokenize, regexp_tokenize, TreebankWordTokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhMe_nSMHbW4",
        "outputId": "7a2d99cb-bf16-4284-b5e2-e09e7e740728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Whitespace Tokenization\n",
        "text = \"LearnWise helps students learn better through personalized content.\"\n",
        "tokens = text.split()\n",
        "print(\"Whitespace Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVADo5KLHgnD",
        "outputId": "8da1f706-352a-48d2-8fcc-f18c6cd70bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokens: ['LearnWise', 'helps', 'students', 'learn', 'better', 'through', 'personalized', 'content.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Regex Tokenization\n",
        "text = \"LearnWise adapts quickly: fast, smart, and scalable!\"\n",
        "tokens = regexp_tokenize(text, r'\\w+|\\$[\\d\\.]+|\\S+')\n",
        "print(\"Regex Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZPyzvgWHiyO",
        "outputId": "899eb20f-6eeb-4d36-c4f7-5294c5998654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regex Tokens: ['LearnWise', 'adapts', 'quickly', ':', 'fast', ',', 'smart', ',', 'and', 'scalable', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. NLTK Word Tokenizer\n",
        "text = \"LearnWise offers adaptive learning for every student.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Word Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACZ9sdaoHkNo",
        "outputId": "7663c1c3-527a-47af-8c87-9de4802f5511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['LearnWise', 'offers', 'adaptive', 'learning', 'for', 'every', 'student', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. spaCy Tokenizer\n",
        "text = \"LearnWise provides rich insights from learner activity.\"\n",
        "doc = nlp(text)\n",
        "tokens = [token.text for token in doc]\n",
        "print(\"spaCy Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D197NM2XHlm2",
        "outputId": "fcb401bd-7025-4740-cecf-1e686141908a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy Tokens: ['LearnWise', 'provides', 'rich', 'insights', 'from', 'learner', 'activity', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Treebank Tokenizer\n",
        "text = \"Students' feedback improves LearnWise's tools.\"\n",
        "tokens = TreebankWordTokenizer().tokenize(text)\n",
        "print(\"Treebank Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YwBGOtfHnOF",
        "outputId": "e8176f3a-71ad-453a-9dad-f1fb35b6d666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treebank Tokens: ['Students', \"'\", 'feedback', 'improves', 'LearnWise', \"'s\", 'tools', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. WordPiece-style Tokenization (simplified)\n",
        "text = \"LearnWise personalization rocks\"\n",
        "subwords = []\n",
        "for word in text.split():\n",
        "    if len(word) > 6:\n",
        "        subwords.append(word[:4])\n",
        "        subwords.append(\"##\" + word[4:])\n",
        "    else:\n",
        "        subwords.append(word)\n",
        "print(\"WordPiece Tokens:\", subwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ytv3r5GaHonA",
        "outputId": "ac4cca22-c454-48a1-d9c1-c4b09b4630dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordPiece Tokens: ['Lear', '##nWise', 'pers', '##onalization', 'rocks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Character Tokenization\n",
        "text = \"Learn\"\n",
        "tokens = list(text)\n",
        "print(\"Character Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atVs9xHlHqS6",
        "outputId": "1509640e-4de9-4a6f-cba6-38b8a911d6ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character Tokens: ['L', 'e', 'a', 'r', 'n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. SentencePiece Tokenization\n",
        "with open(\"learnwise_data.txt\", \"w\") as f:\n",
        "    f.write(\"LearnWise enables adaptive, scalable learning solutions.\")\n",
        "\n",
        "\n",
        "spm.SentencePieceTrainer.Train('--input=learnwise_data.txt --model_prefix=sp --vocab_size=28')\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"sp.model\")\n",
        "\n",
        "text = \"LearnWise personalizes learning.\"\n",
        "tokens = sp.encode(text, out_type=str)\n",
        "print(\"SentencePiece Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1P4m3IxHrrj",
        "outputId": "8ff547ed-cb96-4f07-d01c-3532fb2e830d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentencePiece Tokens: ['▁', 'L', 'earn', 'W', 'i', 's', 'e', '▁', 'p', 'e', 'r', 's', 'o', 'n', 'a', 'l', 'i', 'z', 'e', 's', '▁', 'l', 'earn', 'i', 'n', 'g', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Custom Dictionary Tokenization\n",
        "terms = [\"LearnWise\", \"adaptive learning\", \"feedback\"]\n",
        "text = \"LearnWise supports adaptive learning and real-time feedback.\"\n",
        "\n",
        "for t in sorted(terms, key=len, reverse=True):\n",
        "    text = text.replace(t, t.replace(\" \", \"_\"))\n",
        "\n",
        "tokens = text.split()\n",
        "print(\"Custom Dict Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOF9quM-HtqP",
        "outputId": "f447d047-888e-41d7-c429-643e8fede48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Dict Tokens: ['LearnWise', 'supports', 'adaptive_learning', 'and', 'real-time', 'feedback.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Hybrid Tokenization\n",
        "terms = [\"LearnWise\", \"lesson planner\", \"AI\"]\n",
        "text = \"LearnWise has an AI-powered lesson planner.\"\n",
        "\n",
        "for t in terms:\n",
        "    text = text.replace(t, t.replace(\" \", \"_\"))\n",
        "\n",
        "text = re.sub(r'[^\\w\\s_]', '', text)\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Hybrid Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrG0tGxBHwal",
        "outputId": "892a3e1c-a7ba-4a5b-b38c-db9a9504e485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid Tokens: ['LearnWise', 'has', 'an', 'AIpowered', 'lesson_planner']\n"
          ]
        }
      ]
    }
  ]
}